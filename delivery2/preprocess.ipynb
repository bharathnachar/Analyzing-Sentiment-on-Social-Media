{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df = pd.read_csv('F:\\\\downloads\\\\training.1600000.processed.noemoticon.csv', header=None, encoding='latin')\n",
    "df.columns = ['label', 'id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "# Data reduction\n",
    "df = df.drop(['id', 'date', 'query', 'user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0:'Negative', 2:'Neutral', 4:'Positive'}\n",
    "\n",
    "def convert_labels(label):\n",
    "    return labels_dict[label]\n",
    "\n",
    "df.label = df.label.apply(lambda x: convert_labels(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = df.label.value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(instances.index, instances.values)\n",
    "plt.title(\"Data Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-diagnosis",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "punctuations_and_dummies = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def preprocess(df, will_be_stemmed=False):\n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row.tweet\n",
    "        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in tweet.split():\n",
    "            if token not in stop_words:\n",
    "                if will_be_stemmed:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        df.tweet = \" \".join(tokens)\n",
    "\n",
    "\n",
    "preprocess(df.tweet)\n",
    "'''\n",
    "\n",
    "\n",
    "def preprocess(tweet, will_be_stemmed=False):\n",
    "        tweet = re.sub(punctuations_and_dummies, ' ', str(tweet).lower()).strip()\n",
    "        tokens = []\n",
    "        for token in tweet.split():\n",
    "            if token not in stop_words:\n",
    "                if will_be_stemmed:\n",
    "                    tokens.append(stemmer.stem(token))\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "df.tweet = df.tweet.apply(lambda tw: preprocess(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 0 length tweets\n",
    "df = df[df.iloc[:,1].astype(str).str.len()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_len = [len(x) for x in df['tweet']]\n",
    "pd.Series(tweets_len).hist()\n",
    "plt.show()\n",
    "pd.Series(tweets_len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-demographic",
   "metadata": {},
   "source": [
    "### Number of Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_str = \"\"\n",
    "for i in df.tweet:\n",
    "    all_str += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "letter_list = list(all_str)\n",
    "my_counter = Counter(letter_list)\n",
    "\n",
    "letter_df = pd.DataFrame.from_dict(my_counter, orient='index').reset_index()\n",
    "letter_df = letter_df.rename(columns={'index':'letter', 0:'frequency'})\n",
    "letter_df = letter_df.loc[letter_df['letter'].isin(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])]\n",
    "letter_df['all_tweets_relative_freq']=letter_df['frequency']/letter_df['frequency'].sum()\n",
    "letter_df = letter_df.sort_values('letter')\n",
    "\n",
    "english = pd.read_csv('data/letter_frequency_en_US.csv')\n",
    "english['expected_relative_frequency'] = english['count']/english['count'].sum()\n",
    "english = english.drop(['count'], axis=1)\n",
    "\n",
    "letter_df = pd.merge(letter_df, english, on='letter')\n",
    "letter_df['expected'] = np.round(letter_df['expected_relative_frequency']*letter_df['frequency'].sum(),0)\n",
    "letter_df = letter_df.reset_index().drop(['index'], axis=1)\n",
    "letter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_df.plot(x=\"letter\", y=[\"all_tweets_relative_freq\", \"expected_relative_frequency\"], kind=\"barh\", figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-philadelphia",
   "metadata": {},
   "source": [
    "#### Compare the Observed Frequencies with the Expected Frequencies in English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "# Chi-square test of independence.\n",
    "c, p, dof, expected = chi2_contingency(letter_df[['frequency', 'expected']])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-theta",
   "metadata": {},
   "source": [
    "We get that the p-value (p) is 0 which implies that the letter frequency does not follow the same distribution with what we see in English tests, although the Pearson correlation is too high (~96.7%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-discretion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "letter_df[['frequency', 'expected']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "df1['number_of_characters'] = [len(tw) for tw in df1.tweet]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_characters.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-inspector",
   "metadata": {},
   "source": [
    "## Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['number_of_words'] = [len(tw.split()) for tw in df1.tweet]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.number_of_words.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-matthew",
   "metadata": {},
   "source": [
    "### Positives + Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-gentleman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "all_tweets = ' '.join(df['tweet'].str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "    \n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in whole tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-parks",
   "metadata": {},
   "source": [
    "### Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-peninsula",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(df[df.label == 'Positive'].tweet.str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "    \n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in positive tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 25))\n",
    "plt.axis('off')\n",
    "wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(\" \".join(df[df.label == 'Positive'].tweet))\n",
    "plt.imshow(wordcloud_fig, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-discipline",
   "metadata": {},
   "source": [
    "### Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(df[df.label == 'Negative'].tweet.str.lower())\n",
    "\n",
    "f_words = [word for word in all_tweets.split()]\n",
    "counted_words = collections.Counter(f_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(20):\n",
    "    words.append(letter)\n",
    "    counts.append(count)\n",
    "    \n",
    "plt.figure(figsize = (16, 4))\n",
    "plt.title('Most common words in negative tweets')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.bar(words, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize = (25, 25))\n",
    "plt.axis('off')\n",
    "wordcloud_fig = WordCloud(max_words = 2000 , width = 1600 , height = 800, background_color ='white', min_font_size = 10).generate(\" \".join(df[df.label == 'Negative'].tweet))\n",
    "plt.imshow(wordcloud_fig, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-bundle",
   "metadata": {},
   "source": [
    "### Training Data and Test Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=7)\n",
    "print('Training Data', len(train_data), 'Test Data', len(test_data))\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-success",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data.tweet)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-improvement",
   "metadata": {},
   "source": [
    "### GLOVE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = 'models'\n",
    "EMBEDDING_DIMENSION = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "glove_file = open('glove/glove.6B.300d.txt', encoding='utf8')\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = value = values[0]\n",
    "    coefficients = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefficients\n",
    "glove_file.close()\n",
    "\n",
    "print('%s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIMENSION))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIMENSION, weights=[embedding_matrix], input_length=30, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-emergency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-cabinet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
